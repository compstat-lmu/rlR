---
title: "Define custom environment for deep reinforcement learn"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    dev: svg
vignette: >
  %\VignetteIndexEntry{Define custom environment for deep reinforcement learn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, cache = FALSE}
library(rlR)
set.seed(123)
knitr::opts_chunk$set(cache = TRUE, collapse = FALSE, dev = "svg", fig.height = 3.5)
knitr::knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
library(reticulate)
os = import("os")
os$environ[["TF_CPP_MIN_LOG_LEVEL"]]="3"
```

# rlR: Define custom environments for Deep Reinforcement learning in R

## Environment class

If you want to use this package for your self defined task, you need to implement your own R6 class to represent the environment which must inherit the `rlR::Environment` Class. You could define other public and private members as you like which do not collide with the names in `rlR::Environment`.

```{r}
library(rlR)
help(topic="Environment", package = "rlR")
```

## An Environment that does not make sense

```{r}
library(R6)
MyEnv = R6::R6Class("MyEnv",
  inherit = rlR::Environment,
  public = list(
    step_cnt = NULL,  # a variable that base class does not have
    s_r_d_info = NULL,
    initialize = function(state_dim = 4, act_cnt = 2) {
      super$initialize()
      self$flag_continous = FALSE  # non-continuous action
      self$state_dim = state_dim
      self$act_cnt = act_cnt
      self$step_cnt = 0L
      self$s_r_d_info = list(
            state = array(rnorm(self$state_dim), dim = self$state_dim),
            reward = 1.0,
            done = FALSE,
            info = list()
      )
    },

    render = function(...) {
      # you could leave this field empty. 
    },

    # this function will be called at each step of the learning
    step = function(action) {
      cat(sprintf("calling step %s", self$step_cnt))
      self$step_cnt = self$step_cnt + 1L
      if(self$step_cnt > 5L) {
        self$s_r_d_info[["done"]] = TRUE
        cat(sprintf("\n Episode Over \n"))
        return(self$s_r_d_info)
      }
      return(self$s_r_d_info)
    },

    # this function will be called at the beginning of the learning and at the end of each episode
    reset = function() {
      self$step_cnt = 0
      self$s_r_d_info[["done"]] = FALSE
      self$s_r_d_info
    },

    afterAll = function() {
      print("interaction finished!")
      # what to do after the whole learning is finished?  could be left empty
    }
  )
)
```


Afterwards you could choose one of the available  Agents to learn on this newly defined environments. 
```{r}
env = MyEnv$new()
agent = initAgent("AgentDQN", env)
perf = agent$learn(3)
```
