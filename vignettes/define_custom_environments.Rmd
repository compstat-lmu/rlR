---
title: "Custom Learning Environment"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
    dev: svg
vignette: >
  %\VignetteIndexEntry{Define custom environment for deep reinforcement learn}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, cache = FALSE}
library(rlR)
set.seed(123)
knitr::opts_chunk$set(cache = TRUE, collapse = FALSE, dev = "svg", fig.height = 3.5)
knitr::knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
library(reticulate)
os = import("os") os$environ[["TF_CPP_MIN_LOG_LEVEL"]]="3" ``` 
# rlR: Define Custom Task to solve

## Environment class

If you want to use this package for your self defined task, you need to implement your own R6 class to represent the environment which must inherit the `rlR::Environment` Class. You could define other public and private members as you like which do not collide with the names in `rlR::Environment`.  Type the following to have a look at the documentation of `rlR::Environment`

```{r}
help(topic="Environment", package = "rlR")
```

## A toy Example

```{r}
env = rlR:::EnvToy$new()
```

`rlR:::EnvToy` is an R6 class which inherit  `rlR::Environment`. 

```{r}
class(env)
```

There are 3 methods you must override when defining your own Environment class.

```{r}
env$initialize  # the fields 'act_cnt' and  'state_dim' must be defined here
```

```{r}
env$reset  # The return must be a  list with fields state(must be an array), reward = NULL, done = FALSE, and info = list()
```


```{r}
env$step  # The return must be a list with fields state(must be an array), reward(numeric), done(Boolean), and info (list of anything or empty list)
```

## Testing

Afterwards you could choose one of the available  Agents to check if the newly defined environments works. 

```{r}
agent = initAgent("AgentDQN", env)
agent$learn(3)
```
